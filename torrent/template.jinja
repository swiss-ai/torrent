#!/bin/bash
#SBATCH --job-name={{ job_name }}
#SBATCH --cpus-per-task=288
#SBATCH --gres=gpu:4
#SBATCH --account={{ account }}
#SBATCH --ntasks-per-node=1
#SBATCH --time={{ time }}
#SBATCH --exclusive
#SBATCH --nodes={{ num_nodes_per_worker }}
#SBATCH --ntasks={{ num_nodes_per_worker }}
#SBATCH --partition={{ partition }}
#SBATCH --environment={{ environment }}
#SBATCH --output={{ log_dir }}/log.out
#SBATCH --error={{ log_dir }}/log.err


DP_SIZE={{ dp_size }}
TP_SIZE={{ tp_size }}
EP_SIZE={{ ep_size }}
CUDA_GRAPH_MAX_BS={{ cuda_graph_max_bs }}
GRAMMAR_BACKEND={{ grammar_backend }}
MODEL_PATH={{ model_path }}
NODES_PER_WORKER={{ num_nodes_per_worker }}
LOG_DIR={{ log_dir }}
WORKER_CMD={{ worker_cmd }}
WA_RUN_ID={{ wa_run_id }}
WA_DB_FULL_PATH={{ wa_db_full_path }}
WA_INPUT_DATASET_PATH={{ wa_input_dataset_path }}
WA_INPUT_DATASET_SPLIT={{ wa_input_dataset_split }}
WA_OUTPUT_DATASET_PATH={{ wa_output_dataset_path }}
WA_BATCH_SIZE={{ wa_batch_size }}
WA_MAX_CONCURRENT_REQUESTS={{ wa_max_concurrent_requests }}
WA_TOKEN_USAGE_THRESHOLD={{ wa_token_usage_threshold }}

{% raw %}

worker_id=$1

nodes=($(scontrol show hostnames $SLURM_NODELIST))
if [ ${#nodes[@]} -ne $NODES_PER_WORKER ]; then
    echo "[ERROR]: Expected $NODES_PER_WORKER nodes but got ${#nodes[@]} nodes"
    exit 1
fi

echo "[NODES_IDS]: ${nodes[*]}"

echo "[WORKERS_HEAD_ID]: ${nodes[0]}"

echo "[INFO]: Launching worker $worker_id"

echo "[INFO]: Worker $worker_id nodes: ${nodes[*]}"
worker_host_node=${nodes[0]}

for local_rank in $(seq 0 $((NODES_PER_WORKER - 1))); do
    node=${nodes[$local_rank]}
        
    cmd="srun --nodes=1 --ntasks=1 --nodelist=$node --container-writable --output=${LOG_DIR}/worker${worker_id}_node${local_rank}_${node}.out --error=${LOG_DIR}/worker${worker_id}_node${local_rank}_${node}.err bash ${WORKER_CMD} ${local_rank} ${WA_RUN_ID}<SEP>${SLURM_JOB_ID}<SEP>${WA_DB_FULL_PATH}<SEP>${worker_host_node}<SEP>${WA_INPUT_DATASET_PATH}<SEP>${WA_INPUT_DATASET_SPLIT}<SEP>${WA_OUTPUT_DATASET_PATH}<SEP>${WA_BATCH_SIZE}<SEP>${WA_MAX_CONCURRENT_REQUESTS}<SEP>${WA_TOKEN_USAGE_THRESHOLD} --model-path ${MODEL_PATH} --host 0.0.0.0 --port 30000 --dist-init-addr ${worker_host_node}:5757 --nnodes ${NODES_PER_WORKER} --node-rank ${local_rank} --tp-size ${TP_SIZE} --dp-size ${DP_SIZE} --ep-size ${EP_SIZE} --cuda-graph-max-bs ${CUDA_GRAPH_MAX_BS} --grammar-backend ${GRAMMAR_BACKEND} --decode-log-interval 1 --skip-server-warmup --enable-metrics --random-seed 42"
    echo "[INFO]: $cmd"
    $cmd &
done

echo "[DONE]"

wait
echo "[INFO]: Script finished at $(date)"

{% endraw %}
