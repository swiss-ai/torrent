#!/bin/bash
#SBATCH --job-name={{ job_name }}
#SBATCH --cpus-per-task=288
#SBATCH --gres=gpu:4
#SBATCH --account=a-infra01
#SBATCH --ntasks-per-node=1
#SBATCH --time=04:00:00
#SBATCH --exclusive
#SBATCH --nodes={{ nodes }}
#SBATCH --ntasks={{ nodes }}
#SBATCH --partition={{ partition }}
#SBATCH --environment={{ environment }}
#SBATCH --output=logs/%j/log.out
#SBATCH --error=logs/%j/log.err


DP_SIZE={{ dp_size }}
TP_SIZE={{ tp_size }}
EP_SIZE={{ ep_size }}
CUDA_GRAPH_MAX_BS={{ cuda_graph_max_bs }}
GRAMMAR_BACKEND={{ grammar_backend }}
MODEL_PATH={{ model_path }}
NODES={{ nodes }}
WORKERS={{ workers }}
LOG_DIR="${SLURM_SUBMIT_DIR}/logs/${SLURM_JOB_ID}"
START_SERVER_WITH_ENV_PATH={{ start_server_with_env_path }}

{% raw %}

mkdir -p "${LOG_DIR}"

nodes=($(scontrol show hostnames $SLURM_NODELIST))
if [ ${#nodes[@]} -ne $NODES ]; then
    echo "Error: Expected $NODES nodes but got ${#nodes[@]} nodes"
    exit 1
fi

endpoints="["
for i in $(seq 0 $((WORKERS - 1))); do
    node=${nodes[i]}
    endpoints+="\"http://${node}:5000\""
    if [ $i -lt $((WORKERS - 1)) ]; then
        endpoints+=","
    fi
done
endpoints+="]"
echo "[ENDPOINTS]: $endpoints"
echo ""


# Launch workers
for worker_id in $(seq 0 $((WORKERS - 1))); do
    echo "Launching worker $worker_id"

    node=${nodes[worker_id]}
    
    cmd="srun --nodes=1 --ntasks=1 --nodelist=${node} --container-writable --output=${LOG_DIR}/worker${worker_id}_node_${node}.out --error=${LOG_DIR}/worker${worker_id}_node_${node}.err ${START_SERVER_WITH_ENV_PATH} --model-path ${MODEL_PATH} --host 0.0.0.0 --port 5000 --tp-size ${TP_SIZE} --dp-size ${DP_SIZE} --ep-size ${EP_SIZE} --cuda-graph-max-bs ${CUDA_GRAPH_MAX_BS} --grammar-backend ${GRAMMAR_BACKEND} --decode-log-interval 1"
    echo "$cmd"
    $cmd &
done

echo ""
echo "To connect to the host node:"
echo "srun --jobid $SLURM_JOB_ID -w ${node} --overlap --pty bash"

echo ""
echo "Make sure to cancel the job at the end:"
echo "scancel $SLURM_JOB_ID"

wait
echo "Script finished at $(date)"

{% endraw %}
